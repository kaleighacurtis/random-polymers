{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and preliminary dataframe processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently works when I use my base environment (anaconda3/bin/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#from ax.service.managed_loop import optimize\n",
    "from sklearn import model_selection\n",
    "import json\n",
    "import pandas as pd\n",
    "from scipy.spatial import Delaunay\n",
    "import random\n",
    "from matplotlib import patches\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from shapely.geometry import Point\n",
    "from alphashape import alphashape\n",
    "\n",
    "df = pd.read_csv('no_avg_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqs = np.unique(df[\"Sequence\"])\n",
    "ps = np.unique(df[\"p\"])\n",
    "\n",
    "nn_input_array = np.empty((0,3))\n",
    "nn_output_array = np.empty((0,2))\n",
    "\n",
    "seq_array = np.empty((0,1))\n",
    "\n",
    "seqs_list = []\n",
    "\n",
    "\n",
    "for seq in seqs:\n",
    "    seq_dataset = df[df[\"Sequence\"] == seq]\n",
    "\n",
    "    #Now to get the p=0 point\n",
    "    p0_seq_dataset = seq_dataset[seq_dataset[\"p\"] == 0]\n",
    "    p0_z0 = p0_seq_dataset[\"Z0\"]#.mean()\n",
    "    p0_z1 = p0_seq_dataset[\"Z1\"]#.mean()\n",
    "\n",
    "    #I have the p=0 point, now I need to loop through the p values for each coordinate at that p\n",
    "\n",
    "    for p in ps:\n",
    "        p_seq_dataset = seq_dataset[seq_dataset[\"p\"] == p]\n",
    "        p_z0 = p_seq_dataset[\"Z0\"]#.mean()\n",
    "        p_z1 = p_seq_dataset[\"Z1\"]#.mean()\n",
    "\n",
    "        p_arr = np.full((5,1), p)\n",
    "        #Now I can put everything into the arrays\n",
    "        input_row = np.concatenate((np.asarray(p0_z0).reshape(-1,1), np.asarray(p0_z1).reshape(-1,1), p_arr), axis = 1)\n",
    "        output_row = np.concatenate((np.asarray(p_z0).reshape(-1,1), np.asarray(p_z1).reshape(-1,1)), axis = 1)\n",
    "\n",
    "        seq_row = np.asarray(p_seq_dataset[\"Sequence\"]).reshape(-1,1)\n",
    "\n",
    "        if output_row.shape != (5,2):\n",
    "            # This is because there are around 5 cases where there are only 4 replicas, and I just put this bit in so it would still run while I figured\n",
    "            # out how to edit this block to make it work with varying replica numbers\n",
    "            break\n",
    "    \n",
    "        else:\n",
    "            nn_input_array = np.vstack((nn_input_array, input_row))\n",
    "            nn_output_array = np.vstack((nn_output_array, output_row))\n",
    "\n",
    "            seq_array = np.vstack((seq_array, seq_row))\n",
    "\n",
    "#Arrays with the input and output values for the MLP\n",
    "nia_no_avg = nn_input_array # Net input array\n",
    "noa_no_avg = nn_output_array # Net output array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped splitting, grouped by sequence\n",
    "\n",
    "nia = nia_no_avg\n",
    "noa = noa_no_avg\n",
    "\n",
    "\n",
    "test_val_splitter = GroupShuffleSplit(n_splits = 1, test_size = 0.6, random_state = 0)\n",
    "\n",
    "test_val_indices, train_indices = next(test_val_splitter.split(nia, noa, groups = seq_array))\n",
    "\n",
    "x_test_val, x_train = nia[test_val_indices], nia[train_indices]\n",
    "y_test_val, y_train = noa[test_val_indices], noa[train_indices]\n",
    "groups_test_val = seq_array[test_val_indices]\n",
    "\n",
    "val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_indices, test_indices = next(val_test_splitter.split(x_test_val, y_test_val, groups=groups_test_val))\n",
    "\n",
    "x_val, x_test = x_test_val[val_indices], x_test_val[test_indices]\n",
    "y_val, y_test = y_test_val[val_indices], y_test_val[test_indices]\n",
    "\n",
    "test_seqs = seq_array[test_indices]\n",
    "val_seqs = seq_array[val_indices]\n",
    "#Turning them into tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse_by_p(p, y_pred, x_vals, y_vals):\n",
    "    x_train_subset = []\n",
    "    y_train_subset = []\n",
    "    y_pred_subset = []\n",
    "    for i, point in enumerate(x_vals):\n",
    "        if round(float(point[2]),1) == p:\n",
    "            x_train_subset.append(point)\n",
    "            y_train_subset.append(y_vals[i])\n",
    "            y_pred_subset.append(y_pred[i])\n",
    "    \n",
    "    y_pred_subset = torch.Tensor(torch.stack(y_pred_subset))\n",
    "    y_train_subset = torch.Tensor(torch.stack(y_train_subset))\n",
    "    rmse = torch.sqrt( torch.mean( (y_pred_subset - y_train_subset)**2 ) ).item()\n",
    "\n",
    "    return p, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always run\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=(100, ), activation=nn.ReLU()):\n",
    "        torch.manual_seed(0)  # control random effects\n",
    "\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_layer_size = input_size\n",
    "        for layer_size in hidden_size:\n",
    "            layers.append(nn.Linear(prev_layer_size, layer_size))\n",
    "            layers.append(activation)\n",
    "            prev_layer_size = layer_size\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_size, output_size))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "import tqdm\n",
    "\n",
    "def train_model(model, data, optimizer, n_epochs):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "\n",
    "    # do the training\n",
    "    pbar = tqdm.tqdm(np.arange(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        yp = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = criterion(yp, y_train)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix_str(f'loss: {loss.item():.3e}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create a data structure to convert from str input to nn function\n",
    "activation_dict = {'tanh': nn.Tanh(),\n",
    "                   'relu': nn.ReLU(),\n",
    "                   'leaky_relu': nn.LeakyReLU(),\n",
    "                   'sigmoid': nn.Sigmoid(),\n",
    "                   'elu': nn.ELU()}\n",
    "\n",
    "def generate_model_and_optimizer(params):\n",
    "    # create a list of layer sizes from the start, end, and depth\n",
    "    raw_dims = np.linspace(params[\"layer_i\"], params[\"layer_f\"], params[\"num_layers\"])\n",
    "    hidden_size = tuple(raw_dims.round().astype(int))\n",
    "    # choose the activation function\n",
    "    activation = activation_dict[params['activation']]\n",
    "\n",
    "    # initialize the model\n",
    "    model = MLPRegressor(3, 2, hidden_size, activation)\n",
    "\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "    # return both objects to calling function\n",
    "    return model, optimizer\n",
    "\n",
    "def mlp_fitness(params):\n",
    "    # the evaluation function cannot accept arguments -- we hard-code them here\n",
    "\n",
    "    data = (x_train, x_val, y_train, y_val)\n",
    "    try:\n",
    "        # train on training set\n",
    "        model, optimizer = generate_model_and_optimizer(params)\n",
    "        train_model(model, data, optimizer, params['n_epochs'])\n",
    "        # evaluate on held-out validation set\n",
    "        y_pred = model(x_val).detach()\n",
    "        rmse = torch.sqrt( torch.mean( (y_pred - y_val)**2 ) ).item()\n",
    "    except:\n",
    "        rmse = 1e12\n",
    "    rmse = min(rmse, 1e2)  # need to choose a threshold RMSE to handle errors\n",
    "    return np.log(rmse)    # log will be better behaved for very small numbers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute position model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "data = (x_train, x_val, y_train, y_val)\n",
    "train_model(model, data, optimizer, best_parameters['n_epochs'])\n",
    "# train rmse\n",
    "y_pred = model(x_train).detach()\n",
    "rmse_train = torch.sqrt( torch.mean( (y_pred - y_train)**2 ) ).item()\n",
    "# validation rmse\n",
    "y_pred = model(x_val).detach()\n",
    "rmse_val = torch.sqrt( torch.mean( (y_pred - y_val)**2 ) ).item()\n",
    "# test rmse\n",
    "y_pred = model(x_test).detach()\n",
    "rmse_test = torch.sqrt( torch.mean( (y_pred - y_test)**2 ) ).item()\n",
    "# report\n",
    "print(f'\\ntrain: {rmse_train:.3e} / val: {rmse_val:.3e} / test: {rmse_test:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model and best parameters\n",
    "torch.save(model.state_dict(), 'embedding_model_noavg_absolute.pth')\n",
    "with open('best_parameters_absolute.json', 'w') as f:\n",
    "    json.dump(best_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting RMSE as f(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "#Train\n",
    "y_pred_train = model(x_train).detach()\n",
    "\n",
    "#Test\n",
    "y_pred_test = model(x_test).detach()\n",
    "\n",
    "#Val\n",
    "y_pred_val = model(x_val).detach()\n",
    "\n",
    "train_rmse_list = []\n",
    "test_rmse_list = []\n",
    "val_rmse_list = []\n",
    "\n",
    "for p in ps:\n",
    "    _, rmse_train = get_rmse_by_p(p, y_pred_train, x_train, y_train)\n",
    "    _, rmse_test = get_rmse_by_p(p, y_pred_test, x_test, y_test)\n",
    "    _, rmse_val = get_rmse_by_p(p, y_pred_val, x_val, y_val)\n",
    "\n",
    "    train_rmse_list += [rmse_train]\n",
    "    test_rmse_list += [rmse_test]\n",
    "    val_rmse_list +=[rmse_val]\n",
    "\n",
    "    print(f'\\n p: {p}  train: {rmse_train:.3e} / val: {rmse_val:.3e} / test: {rmse_test:.3e}')\n",
    "\n",
    "plt.plot(ps, train_rmse_list, label = \"Train\")\n",
    "plt.plot(ps, test_rmse_list, label = \"Test\")\n",
    "plt.plot(ps, val_rmse_list, label = \"Validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Absolute Position Model RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "#Initializing the model (maybe not the right wording here)\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "model_dict = torch.load('embedding_model_noavg_absolute.pth')\n",
    "\n",
    "#Loading pre-saved weights to the model\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative position model (vector model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vector data\n",
    "\n",
    "def directional_vector(point1, point2):\n",
    "    \"\"\"Calculate the directional vector from point1 to point2.\"\"\"\n",
    "    return (point2[0] - point1[0], point2[1] - point1[1])\n",
    "\n",
    "df = pd.read_csv('embedding_data.csv')\n",
    "\n",
    "df_vector_list = []\n",
    "bigx = []\n",
    "bigy = []\n",
    "bigp = []\n",
    "bigseq = []\n",
    "\n",
    "p0_x = []\n",
    "p0_y = []\n",
    "ps = np.unique(df[\"p\"])\n",
    "\n",
    "for seq in np.unique(df[\"Sequence\"]):\n",
    "    sub_seq_data = df[df[\"Sequence\"] == seq]\n",
    "    seq_points_x = []\n",
    "    seq_points_y = []\n",
    "\n",
    "    sub_p0 = sub_seq_data[sub_seq_data[\"p\"] == 0 ]\n",
    "    z0_p0_mean = sub_p0[\"Z0\"].mean()\n",
    "    z1_p0_mean = sub_p0[\"Z1\"].mean()\n",
    "    for p in ps:\n",
    "        sub2 = sub_seq_data[sub_seq_data[\"p\"] == p]\n",
    "        z0_mean = sub2[\"Z0\"].mean()\n",
    "        z1_mean = sub2[\"Z1\"].mean()\n",
    "        seq_points_x.append(z0_mean)\n",
    "        seq_points_y.append(z1_mean)\n",
    "        bigp += [p]\n",
    "        #Need to get the p=0 points here\n",
    "        p0_x += [z0_p0_mean]\n",
    "        p0_y += [z1_p0_mean]\n",
    "        bigseq +=[seq]\n",
    "\n",
    "\n",
    "    bigx+= seq_points_x\n",
    "    bigy += seq_points_y\n",
    "    coordinates = list(zip(seq_points_x, seq_points_y))\n",
    "    p0_coord = (z0_p0_mean, z1_p0_mean)\n",
    "    vectors = [directional_vector(coordinates[i], p0_coord) for i in range(len(coordinates))]\n",
    "    df_vector_list += vectors\n",
    "    \n",
    "start_point_and_p = [] #Getting the start point for the vector and the p value for that point\n",
    "\n",
    "\n",
    "for row in list(zip(p0_x, p0_y, bigp)):\n",
    "    start_point_and_p += [row]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test val splitting for vector data\n",
    "test_val_splitter = GroupShuffleSplit(n_splits = 1, test_size = 0.6, random_state = 0)\n",
    "\n",
    "#DIFFERENT FROM ABSOLUTE MODEL\n",
    "nia = np.array(start_point_and_p)\n",
    "noa = np.array(df_vector_list)\n",
    "seq_array = np.array(bigseq)\n",
    "\n",
    "test_val_indices, train_indices = next(test_val_splitter.split(nia, noa, groups = seq_array))\n",
    "\n",
    "x_test_val, x_train_rel = nia[test_val_indices], nia[train_indices]\n",
    "y_test_val, y_train_rel = noa[test_val_indices], noa[train_indices]\n",
    "groups_test_val = seq_array[test_val_indices]\n",
    "\n",
    "val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_indices, test_indices = next(val_test_splitter.split(x_test_val, y_test_val, groups=groups_test_val))\n",
    "\n",
    "x_val_rel, x_test_rel = x_test_val[val_indices], x_test_val[test_indices]\n",
    "y_val_rel, y_test_rel = y_test_val[val_indices], y_test_val[test_indices]\n",
    "\n",
    "test_seqs = seq_array[test_indices]\n",
    "val_seqs = seq_array[val_indices]\n",
    "#Turning them into tensors\n",
    "x_train_rel = torch.Tensor(x_train_rel)\n",
    "x_val_rel = torch.Tensor(x_val_rel)\n",
    "y_train_rel = torch.Tensor(y_train_rel)\n",
    "y_val_rel = torch.Tensor(y_val_rel)\n",
    "x_test_rel = torch.Tensor(x_test_rel)\n",
    "y_test_rel = torch.Tensor(y_test_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model_relative, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "x_train_rel = torch.Tensor(x_train_rel)\n",
    "x_val_rel = torch.Tensor(x_val_rel)\n",
    "y_train_rel = torch.Tensor(y_train_rel)\n",
    "y_val_rel = torch.Tensor(y_val_rel)\n",
    "x_test_rel = torch.Tensor(x_test_rel)\n",
    "y_test_rel = torch.Tensor(y_test_rel)\n",
    "\n",
    "data = (x_train_rel, x_val_rel, y_train_rel, y_val_rel)\n",
    "train_model(model_relative, data, optimizer, best_parameters['n_epochs'])\n",
    "# train rmse\n",
    "y_pred = model_relative(x_train_rel).detach()\n",
    "rmse_train = torch.sqrt( torch.mean( (y_pred - y_train_rel)**2 ) ).item()\n",
    "# validation rmse\n",
    "y_pred = model_relative(x_val_rel).detach()\n",
    "rmse_val = torch.sqrt( torch.mean( (y_pred - y_val_rel)**2 ) ).item()\n",
    "# test rmse\n",
    "y_pred = model_relative(x_test_rel).detach()\n",
    "rmse_test = torch.sqrt( torch.mean( (y_pred - y_test_rel)**2 ) ).item()\n",
    "# report\n",
    "print(f'\\ntrain: {rmse_train:.3e} / val: {rmse_val:.3e} / test: {rmse_test:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model \n",
    "#Saving model and best parameters\n",
    "torch.save(model.state_dict(), 'embedding_model_noavg_relative.pth')\n",
    "with open('best_parameters_relative.json', 'w') as f:\n",
    "    json.dump(best_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding RMSE as a f(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rel = torch.Tensor(x_train_rel)\n",
    "x_val_rel = torch.Tensor(x_val_rel)\n",
    "y_train_rel = torch.Tensor(y_train_rel)\n",
    "y_val_rel = torch.Tensor(y_val_rel)\n",
    "x_test_rel = torch.Tensor(x_test_rel)\n",
    "y_test_rel = torch.Tensor(y_test_rel)\n",
    "\n",
    "#Train\n",
    "y_pred_train = model_relative(x_train_rel).detach()\n",
    "\n",
    "#Test\n",
    "y_pred_test = model_relative(x_test_rel).detach()\n",
    "\n",
    "#Val\n",
    "y_pred_val = model_relative(x_val_rel).detach()\n",
    "\n",
    "train_rmse_list = []\n",
    "test_rmse_list = []\n",
    "val_rmse_list = []\n",
    "\n",
    "for p in ps:\n",
    "    _, rmse_train = get_rmse_by_p(p, y_pred_train, x_train_rel, y_train_rel)\n",
    "    _, rmse_test = get_rmse_by_p(p, y_pred_test, x_test_rel, y_test_rel)\n",
    "    _, rmse_val = get_rmse_by_p(p, y_pred_val, x_val_rel, y_val_rel)\n",
    "\n",
    "    train_rmse_list += [rmse_train]\n",
    "    test_rmse_list += [rmse_test]\n",
    "    val_rmse_list +=[rmse_val]\n",
    "\n",
    "    print(f'\\n p: {p}  train: {rmse_train:.3e} / val: {rmse_val:.3e} / test: {rmse_test:.3e}')\n",
    "\n",
    "plt.plot(ps, train_rmse_list, label = \"Train\")\n",
    "plt.plot(ps, test_rmse_list, label = \"Test\")\n",
    "plt.plot(ps, val_rmse_list, label = \"Validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Relative Position Model RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To load the model without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to reload the model without training\n",
    "# If I want to load the model without retraining:\n",
    "with open('best_parameters_relative.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "#Initializing the model (maybe not the right wording here)\n",
    "model_relative, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "model_dict = torch.load('embedding_model_noavg_relative.pth')\n",
    "\n",
    "#Loading pre-saved weights to the model\n",
    "model_relative.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading absolute model\n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "#Initializing the model (maybe not the right wording here)\n",
    "model_absolute, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "model_dict = torch.load('embedding_model_noavg_absolute.pth')\n",
    "\n",
    "#Loading pre-saved weights to the model\n",
    "model_absolute.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get grid points\n",
    "\n",
    "from alphashape import alphashape\n",
    "\n",
    "ps = np.unique(df[\"p\"])\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "max_x = max(z_array[:,0])\n",
    "max_y = max(z_array[:,1])\n",
    "\n",
    "min_x = min(z_array[:,0])\n",
    "min_y = min(z_array[:,1])\n",
    "\n",
    "x_range = np.linspace(min_x, max_x, 30)\n",
    "y_range = np.linspace(min_y, max_y, 30)\n",
    "X,Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "mesh_point_set = []\n",
    "\n",
    "#Making an array of all the points\n",
    "for i, row in enumerate(X): \n",
    "    row_mesh = np.concatenate((row.reshape(-1,1), Y[i].reshape(-1,1)), axis = 1)\n",
    "    mesh_point_set.append(row_mesh)\n",
    "\n",
    "mesh_set = np.vstack(mesh_point_set)\n",
    "\n",
    "\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "#getting mesh points that are actually in my alpha shape\n",
    "alpha_shape_mesh_points = []\n",
    "for point in mesh_set:\n",
    "    point_shape = Point(point)\n",
    "    if alpha_shape.contains(point_shape) == True:\n",
    "        alpha_shape_mesh_points.append(point)\n",
    "alpha_shape_mesh = np.vstack(alpha_shape_mesh_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding norms and plotting\n",
    "\n",
    "zero_arr = np.full((alpha_shape_mesh.shape[0], 1), 0)\n",
    "grid_pts = np.hstack((alpha_shape_mesh, zero_arr))\n",
    "\n",
    "row_and_norm = []\n",
    "\n",
    "for row in grid_pts:\n",
    "    point = torch.Tensor(row)\n",
    "    point.requires_grad_(True)\n",
    "    output = model_absolute(point)\n",
    "    scalar_output = torch.sum(output)\n",
    "    scalar_output.backward()\n",
    "    dx = point.grad[0].item()\n",
    "    dy = point.grad[1].item()\n",
    "    gradient_norm = torch.norm(point.grad)\n",
    "\n",
    "    x = point[0].detach().numpy()\n",
    "    y = point[1].detach().numpy()\n",
    "    norm = gradient_norm.detach().numpy()\n",
    "\n",
    "    row_and_norm += [np.array((x, y, norm))]\n",
    "\n",
    "row_and_norm_arr = np.vstack(row_and_norm)\n",
    "plt.scatter(row_and_norm_arr[:,0], row_and_norm_arr[:,1], c = row_and_norm_arr[:,2], cmap = 'viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Norm of gradients using absolute model\")\n",
    "plt.xlabel(\"Z0\")\n",
    "plt.ylabel(\"Z1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to reload the model without training\n",
    "# If I want to load the model without retraining:\n",
    "with open('best_parameters_relative.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "#Initializing the model (maybe not the right wording here)\n",
    "model_relative, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "model_dict = torch.load('embedding_model_noavg_relative.pth')\n",
    "\n",
    "#Loading pre-saved weights to the model\n",
    "model_relative.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get grid points\n",
    "\n",
    "from alphashape import alphashape\n",
    "\n",
    "ps = np.unique(df[\"p\"])\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "max_x = max(z_array[:,0])\n",
    "max_y = max(z_array[:,1])\n",
    "\n",
    "min_x = min(z_array[:,0])\n",
    "min_y = min(z_array[:,1])\n",
    "\n",
    "x_range = np.linspace(min_x, max_x, 30)\n",
    "y_range = np.linspace(min_y, max_y, 30)\n",
    "X,Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "mesh_point_set = []\n",
    "\n",
    "#Making an array of all the points\n",
    "for i, row in enumerate(X): \n",
    "    row_mesh = np.concatenate((row.reshape(-1,1), Y[i].reshape(-1,1)), axis = 1)\n",
    "    mesh_point_set.append(row_mesh)\n",
    "\n",
    "mesh_set = np.vstack(mesh_point_set)\n",
    "\n",
    "\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "#getting mesh points that are actually in my alpha shape\n",
    "alpha_shape_mesh_points = []\n",
    "for point in mesh_set:\n",
    "    point_shape = Point(point)\n",
    "    if alpha_shape.contains(point_shape) == True:\n",
    "        alpha_shape_mesh_points.append(point)\n",
    "alpha_shape_mesh = np.vstack(alpha_shape_mesh_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_arr = np.full((alpha_shape_mesh.shape[0], 1), 0)\n",
    "grid_pts = np.hstack((alpha_shape_mesh, zero_arr))\n",
    "\n",
    "row_and_norm = []\n",
    "\n",
    "for row in grid_pts:\n",
    "    point = torch.Tensor(row)\n",
    "    point.requires_grad_(True)\n",
    "    output = model_relative(point)\n",
    "    scalar_output = torch.sum(output)\n",
    "    scalar_output.backward()\n",
    "    dx = point.grad[0].item()\n",
    "    dy = point.grad[1].item()\n",
    "    gradient_norm = torch.norm(point.grad)\n",
    "\n",
    "    x = point[0].detach().numpy()\n",
    "    y = point[1].detach().numpy()\n",
    "    norm = gradient_norm.detach().numpy()\n",
    "\n",
    "    row_and_norm += [np.array((x, y, norm))]\n",
    "\n",
    "row_and_norm_arr = np.vstack(row_and_norm)\n",
    "plt.scatter(row_and_norm_arr[:,0], row_and_norm_arr[:,1], c = row_and_norm_arr[:,2], cmap = 'viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Norm of gradients using relative/vector model\")\n",
    "plt.xlabel(\"Z0\")\n",
    "plt.ylabel(\"Z1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uniform sampling in p space and xy space, they'll be different. \n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "lines = []\n",
    "\n",
    "for seq in np.unique(df[\"Sequence\"]):\n",
    "    sub_seq_data = df[df[\"Sequence\"] == seq]\n",
    "    for run in np.unique(df[\"Run number\"]):\n",
    "        sub_run_data = sub_seq_data[sub_seq_data[\"Run number\"] == run]\n",
    "\n",
    "        x = sub_run_data[\"Z0\"]\n",
    "        y = sub_run_data[\"Z1\"]\n",
    "        cols = sub_run_data[\"p\"]\n",
    "\n",
    "        line = np.vstack((x,y))\n",
    "        lines += [line]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting points evently along a streamline, evenly along x and y \n",
    "\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "for seq in np.unique(df[\"Sequence\"]):\n",
    "    sub_seq_data = df[df[\"Sequence\"] == seq]\n",
    "    for run in np.unique(df[\"Run number\"]):\n",
    "        sub_run_data = sub_seq_data[sub_seq_data[\"Run number\"] == run]\n",
    "\n",
    "        x = sub_run_data[\"Z0\"]\n",
    "        y = sub_run_data[\"Z1\"]\n",
    "        cols = sub_run_data[\"p\"]\n",
    "\n",
    "        points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "\n",
    "            \n",
    "        segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "        all_lines +=[segments]\n",
    "            \n",
    "\n",
    "sampled_lines_all = []\n",
    "for idk in all_lines:\n",
    "\n",
    "# Example: Replace this with your actual line segments\n",
    "    line_segments = np.array(idk)\n",
    "    # Convert line segments to LineString objects\n",
    "    # Interpolate points along each line segment\n",
    "    sampled_points = []\n",
    "    for segment in line_segments:\n",
    "        line = LineString(segment)\n",
    "        total_length = line.length\n",
    "        desired_interval = 0.5  # Adjust as needed\n",
    "\n",
    "        current_length = 0\n",
    "        while current_length < total_length:\n",
    "            point = line.interpolate(current_length)\n",
    "            x, y = point.xy[0][0], point.xy[1][0]\n",
    "            sampled_points.append((x, y))\n",
    "            current_length += desired_interval\n",
    "\n",
    "    # Convert sampled points to a NumPy array\n",
    "    sampled_points_array = np.array(sampled_points)\n",
    "    sampled_lines_all +=[sampled_points_array]\n",
    "\n",
    "# Plot the original line segments\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "for segment in line_segments:\n",
    "    ax.plot(segment[:, 0], segment[:, 1], color='blue', marker='o')\n",
    "\n",
    "# Plot the sampled points\n",
    "ax.scatter(sampled_points_array[:, 0], sampled_points_array[:, 1], color='red', marker='x')\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "plt.plot(*alpha_shape.exterior.xy, color='black', linewidth=2,alpha = 0.7, label='Alpha Shape')\n",
    "plt.title(\"Interval = \" + str(desired_interval))\n",
    "plt.show()\n",
    "\n",
    "even_xy_mesh = np.vstack(sampled_lines_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "#Get grid points\n",
    "\n",
    "from alphashape import alphashape\n",
    "\n",
    "ps = np.unique(df[\"p\"])\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "max_x = max(z_array[:,0])\n",
    "max_y = max(z_array[:,1])\n",
    "\n",
    "min_x = min(z_array[:,0])\n",
    "min_y = min(z_array[:,1])\n",
    "\n",
    "x_range = np.linspace(min_x, max_x, 50)\n",
    "y_range = np.linspace(min_y, max_y, 50)\n",
    "X,Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "mesh_point_set = []\n",
    "\n",
    "#Making an array of all the points\n",
    "for i, row in enumerate(X): \n",
    "    row_mesh = np.concatenate((row.reshape(-1,1), Y[i].reshape(-1,1)), axis = 1)\n",
    "    mesh_point_set.append(row_mesh)\n",
    "\n",
    "mesh_set = np.vstack(mesh_point_set)\n",
    "\n",
    "\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "#getting mesh points that are actually in my alpha shape\n",
    "alpha_shape_mesh_points = []\n",
    "for point in mesh_set:\n",
    "    point_shape = Point(point)\n",
    "    if alpha_shape.contains(point_shape) == True:\n",
    "        alpha_shape_mesh_points.append(point)\n",
    "alpha_shape_mesh = np.vstack(alpha_shape_mesh_points)\n",
    "\n",
    "\n",
    "empty_list = []\n",
    "\n",
    "for line in lines:\n",
    "    idk= np.vstack((line[0], line[1])).T\n",
    "    empty_list += [idk]\n",
    "\n",
    "#lines = np.asarray(lines)\n",
    "\n",
    "# Flatten the arrays and concatenate\n",
    "#all_points = np.concatenate([line.T for line in lines])\n",
    "\n",
    "all_points = empty_list\n",
    "\n",
    "your_mesh = even_xy_mesh\n",
    "\n",
    "# Perform kernel density estimation\n",
    "kde = gaussian_kde(your_mesh.T)\n",
    "\n",
    "#grid_coordinates = alpha_shape_mesh\n",
    "\n",
    "density_values = kde(your_mesh.T)\n",
    "\n",
    "# Reshape the density values to match the shape of your_mesh\n",
    "\n",
    "\n",
    "# Plot the original lines\n",
    "\n",
    "#for line in lines:\n",
    "        #plt.plot(line[0, :], line[1, :], color='gray', alpha=0.2, zorder = 0)\n",
    "\n",
    "line = lines[0]\n",
    "plt.plot(line[0, :], line[1, :], color='gray', alpha=0.2, zorder = 0)\n",
    "\n",
    "# Plot the KDE density\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "plt.plot(*alpha_shape.exterior.xy, color='black', linewidth=2,alpha = 0.7, label='Alpha Shape')\n",
    "\n",
    "plt.scatter(your_mesh[:, 0], your_mesh[:, 1], c=density_values, cmap='viridis', marker='.', s=10, zorder = 1)\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Kernel Density Estimation for Lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE for even spacing along p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_run_df = df[df[\"Run number\"] == 1]\n",
    "downselected_seq = np.random.choice(np.unique(one_run_df[\"Sequence\"]), size = 70, replace = False)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for thing in downselected_seq:\n",
    "    sampleset = one_run_df[one_run_df[\"Sequence\"] == thing]\n",
    "    x = list(sampleset[\"Z0\"])\n",
    "    y = list(sampleset[\"Z1\"])\n",
    "\n",
    "    xs += [x]\n",
    "    ys += [y]\n",
    "\n",
    "x_arr = np.hstack(xs)\n",
    "y_arr = np.hstack(ys)\n",
    "\n",
    "streamline_grid = np.vstack((x_arr, y_arr)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get grid points\n",
    "\n",
    "from alphashape import alphashape\n",
    "\n",
    "ps = np.unique(df[\"p\"])\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "\n",
    "alpha_shape_mesh = streamline_grid\n",
    "\n",
    "empty_list = []\n",
    "\n",
    "for line in lines:\n",
    "    idk= np.vstack((line[0], line[1])).T\n",
    "    empty_list += [idk]\n",
    "\n",
    "#lines = np.asarray(lines)\n",
    "\n",
    "# Flatten the arrays and concatenate\n",
    "#all_points = np.concatenate([line.T for line in lines])\n",
    "\n",
    "all_points = empty_list\n",
    "\n",
    "your_mesh = alpha_shape_mesh\n",
    "\n",
    "# Perform kernel density estimation\n",
    "kde = gaussian_kde(your_mesh.T)\n",
    "\n",
    "#grid_coordinates = alpha_shape_mesh\n",
    "\n",
    "density_values = kde(your_mesh.T)\n",
    "\n",
    "# Reshape the density values to match the shape of your_mesh\n",
    "\n",
    "\n",
    "# Plot the original lines\n",
    "\n",
    "for line in lines:\n",
    "        plt.plot(line[0, :], line[1, :], color='gray', alpha=0.05)\n",
    "\n",
    "# Plot the KDE density\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "plt.plot(*alpha_shape.exterior.xy, color='black', linewidth=2,alpha = 0.7, label='Alpha Shape')\n",
    "\n",
    "plt.scatter(your_mesh[:, 0], your_mesh[:, 1], c=density_values, cmap='viridis', marker='.', s=10)\n",
    "plt.colorbar(label='Density')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Kernel Density Estimation for Lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hex grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gets the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First go at making hex grid points and finding neighbors\n",
    "\n",
    "def create_hex_grid(radius, x_min, y_min, x_max, y_max):\n",
    "    \"\"\"\n",
    "    Create a hex grid of points within a specified bounding box.\n",
    "\n",
    "    Parameters:\n",
    "    - radius: Radius of the hexagon.\n",
    "    - x_min: Minimum x-coordinate of the bounding box.\n",
    "    - y_min: Minimum y-coordinate of the bounding box.\n",
    "    - x_max: Maximum x-coordinate of the bounding box.\n",
    "    - y_max: Maximum y-coordinate of the bounding box.\n",
    "\n",
    "    Returns:\n",
    "    - List of hex grid points.\n",
    "    \"\"\"\n",
    "    hex_points = []\n",
    "    hex_width = radius * np.sqrt(3)\n",
    "    hex_height = 2 * radius\n",
    "\n",
    "    # Calculate the number of rows and columns based on the bounding box\n",
    "    num_rows = int((y_max - y_min) / (1.5 * radius))\n",
    "    num_cols = int((x_max - x_min) / hex_width)\n",
    "\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            x = col * hex_width + x_min\n",
    "            y = row * 1.5 * radius + y_min\n",
    "\n",
    "            # Offset every other row\n",
    "            if col % 2 == 1:\n",
    "                y += 0.75 * radius\n",
    "\n",
    "            hex_points.append((x, y))\n",
    "\n",
    "    return hex_points\n",
    "\n",
    "#Parameters for hex grid and getting hex grid on a rectangle\n",
    "radius = 1.0\n",
    "x_min, y_min, x_max, y_max = -7.0, -7.0, 25.0, 15.0\n",
    "hex_points = create_hex_grid(radius, x_min, y_min, x_max, y_max)\n",
    "\n",
    "#Getting mesh points that are actually in my alpha shape\n",
    "alpha_shape_mesh_points = []\n",
    "for point in hex_points:\n",
    "    point_shape = Point(point)\n",
    "    if alpha_shape.contains(point_shape) == True:\n",
    "        alpha_shape_mesh_points.append(point)\n",
    "alpha_shape_mesh_hex = np.vstack(alpha_shape_mesh_points)\n",
    "\n",
    "\n",
    "#Plotting hex grid\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "plt.plot(*alpha_shape.exterior.xy, color='black', linewidth=2,alpha = 0.7, label='Alpha Shape')\n",
    "plt.scatter(alpha_shape_mesh_hex[:,0], alpha_shape_mesh_hex[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the closest hex grid points to a certain point inside the alphashape\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "def get_neighbors(center, all_points, radius, alphashape):\n",
    "    neighbors = []\n",
    "\n",
    "    for point in all_points:\n",
    "        if not np.array_equal(point, center) and euclidean_distance(center, point) <= radius:\n",
    "            neighbors.append(point)\n",
    "\n",
    "    neighbor_mesh = []\n",
    "    for point in neighbors:\n",
    "        point_shape = Point(point)\n",
    "        if alphashape.contains(point_shape) == True:\n",
    "            neighbor_mesh.append(point)\n",
    "    neighbor_mesh = np.vstack(neighbor_mesh)\n",
    "\n",
    "    return neighbor_mesh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Choose a center point for which you want to find neighbors\n",
    "center_point = alpha_shape_mesh_hex[25]\n",
    "radius = 2\n",
    "\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "# Get neighbors of the center point within the specified radius\n",
    "neighbors = get_neighbors(center_point, hex_points, radius, alpha_shape)\n",
    "\n",
    "print(f\"Neighbors of {center_point} in alphashape: {neighbors}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(*alpha_shape.exterior.xy, color='black', linewidth=2,alpha = 0.7, label='Alpha Shape')\n",
    "plt.scatter(center_point[0], center_point[1])\n",
    "plt.scatter(neighbors[:,0], neighbors[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have hex grid\n",
    "\n",
    "# Loops through every point in mesh\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))\n",
    "\n",
    "def get_neighbors(center, all_points, radius, alphashape):\n",
    "    neighbors = []\n",
    "\n",
    "    for point in all_points:\n",
    "        if not np.array_equal(point, center) and euclidean_distance(center, point) <= radius:\n",
    "            neighbors.append(point)\n",
    "\n",
    "    neighbor_mesh = []\n",
    "    for point in neighbors:\n",
    "        point_shape = Point(point)\n",
    "        if alphashape.contains(point_shape) == True:\n",
    "            neighbor_mesh.append(point)\n",
    "    neighbor_mesh = np.vstack(neighbor_mesh)\n",
    "\n",
    "    return neighbor_mesh\n",
    "\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "radius = 2\n",
    "\n",
    "for center_point in alpha_shape_mesh_hex:\n",
    "    neighbors = get_neighbors(center_point, hex_points, radius, alpha_shape)\n",
    "    print(f\"Neighbors of {center_point}: {neighbors}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
